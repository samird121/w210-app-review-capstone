{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import gensim, logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.spatial.distance import cdist, cosine\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from bokeh.io import curdoc\n",
    "from bokeh.layouts import row, widgetbox\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models.widgets import Slider, TextInput\n",
    "from bokeh.plotting import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./android/ebay/total_info.txt') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    d = list(reader)\n",
    "\n",
    "ratings = []\n",
    "reviews = []\n",
    "titles = []\n",
    "dates = []\n",
    "versions = []\n",
    "\n",
    "for line in d:\n",
    "    vals = line[0].split(\"******\")\n",
    "    ratings.append(float(vals[0]))\n",
    "    reviews.append(vals[1])\n",
    "    dates.append(vals[2])\n",
    "    versions.append(vals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "def custom_preprocessor(text):\n",
    "    porter = PorterStemmer()\n",
    "\n",
    "    #split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    final_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_split = sentence.split(\" \")\n",
    "        \n",
    "        #remove words in not in stop words, and make lowercase\n",
    "        words = [word.lower() for word in sentence_split if word.lower() not in stop_words]\n",
    "        #get rid of words with non alphanumeric characters in it\n",
    "        #(should we replace these with a token?)\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "        #stem words\n",
    "        words = [porter.stem(word) for word in words]\n",
    "\n",
    "        final_sentences.append(\" \".join(words))\n",
    "        \n",
    "        #consider joining sentences with a stop token\n",
    "    return \" \".join(final_sentences), final_sentences, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviews = []\n",
    "processed_sentences = []\n",
    "raw_sentences = []\n",
    "for review in reviews:\n",
    "    processed_review = custom_preprocessor(review) \n",
    "    processed_reviews.append(processed_review[0])\n",
    "    processed_sentences.append(processed_review[1])\n",
    "    raw_sentences.append(processed_review[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of reviews that are empty after preprocessing\n",
    "#(not that many)\n",
    "\n",
    "processed_review_lens = np.array([len(review) for review in [r.split(\" \") for r in processed_reviews]])\n",
    "#if using stop tokens \"<END>\" then empty reviews have a length of 6\n",
    "nonzero_indeces = np.where(processed_review_lens > 0)\n",
    "\n",
    "\n",
    "final_reviews =  [review.split(\" \") for review in np.array(processed_reviews)[nonzero_indeces]]\n",
    "final_reviews_unprocessed =  np.array(reviews)[nonzero_indeces]\n",
    "final_ratings = np.array(ratings)[nonzero_indeces]\n",
    "#final_titles = np.array(titles)[nonzero_indeces]\n",
    "final_dates = np.array(dates)[nonzero_indeces]\n",
    "unique_dates = np.unique(np.array(final_dates))\n",
    "unique_date_indices = []\n",
    "for date in unique_dates:\n",
    "        date_indices = np.where(np.array(final_dates)==date)[0]\n",
    "        unique_date_indices.append(date_indices)\n",
    "final_versions = np.array(versions)[nonzero_indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8001821041107178\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = Word2Vec(final_reviews, min_count=1)\n",
    "#model.save(\"../../large files/youtube_w2v_stoptokens.model\")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we create a vector for each review,\n",
    "#which will be the simple average of all word vectors in that review.\n",
    "#these vectors will then be used for clustering, data reduction, etc.\n",
    "avg_vectors = []\n",
    "for review in final_reviews:\n",
    "    avg_vectors.append(np.mean([model.wv[word] for word in review], axis=0))\n",
    "    \n",
    "avg_vectors = np.array(avg_vectors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_review_lengths = [len(review.split(\" \")) for review in final_reviews_unprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35483, 100)\n",
      "(35483, 100)\n"
     ]
    }
   ],
   "source": [
    "#scaling \n",
    "scaler = StandardScaler()\n",
    "avg_vectors_scaled = scaler.fit_transform(avg_vectors)\n",
    "print(avg_vectors.shape)\n",
    "print(avg_vectors_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_reviews(key, avg_vectors, raw_text, n=1, scaled=False):\n",
    "    \n",
    "    indices = None\n",
    "    distances = None\n",
    "    \n",
    "    key_processed = custom_preprocessor(key)[0]\n",
    "    key_list = key_processed.split(' ')\n",
    "    #filter to only those in the w2v model's covacbulary\n",
    "    vocab = model.wv.vocab.keys()\n",
    "    key_list_vocab_words = []\n",
    "    key_list_nonvocab_words = []\n",
    "    for word in key_list:\n",
    "        if word in vocab:\n",
    "            key_list_vocab_words.append(word)\n",
    "        else:\n",
    "            key_list_nonvocab_words.append(word)\n",
    "    \n",
    "    #only move on if the list isn't empty (i.e. if the key had no words in the vocab)\n",
    "    if len(key_list_vocab_words) > 0:\n",
    "        key_vector = np.mean([model.wv[word] for word in key_list_vocab_words if word in model.wv.vocab.keys()], axis=0)\n",
    "        \n",
    "        if scaled:\n",
    "            key_vector = scaler.transform(np.array(key_vector).reshape(1,-1))\n",
    "\n",
    "        distances = [1-cosine(key_vector, vector) for vector in avg_vectors]\n",
    "        indices = np.argsort(distances)[-n:]\n",
    "    else:\n",
    "        print(\"Warning: none of the words in the query were in the model's vocabulary.\")\n",
    "    \n",
    "    if len(key_list_nonvocab_words) > 0:\n",
    "        print(\"Excluded words:\", key_list_nonvocab_words)\n",
    "        \n",
    "    return indices, distances\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data\n",
    "N = 200\n",
    "x = np.linspace(0, 4*np.pi, N)\n",
    "y = np.sin(x)\n",
    "source = ColumnDataSource(data=dict(x=x, y=y))\n",
    "\n",
    "\n",
    "# Set up plot\n",
    "plot = figure(plot_height=400, plot_width=400, title=\"my sine wave\",\n",
    "              tools=\"crosshair,pan,reset,save,wheel_zoom\",\n",
    "              x_range=[0, 4*np.pi], y_range=[-2.5, 2.5])\n",
    "\n",
    "plot.line('x', 'y', source=source, line_width=3, line_alpha=0.6)\n",
    "\n",
    "\n",
    "# Set up widgets\n",
    "query = TextInput(title=\"query\", value='i love this app')\n",
    "offset = Slider(title=\"offset\", value=0.0, start=-5.0, end=5.0, step=0.1)\n",
    "amplitude = Slider(title=\"amplitude\", value=1.0, start=-5.0, end=5.0, step=0.1)\n",
    "phase = Slider(title=\"phase\", value=0.0, start=0.0, end=2*np.pi)\n",
    "freq = Slider(title=\"frequency\", value=1.0, start=0.1, end=5.1, step=0.1)\n",
    "\n",
    "\n",
    "# Set up callbacks\n",
    "def update_title(attrname, old, new):\n",
    "    plot.title.text = text.value\n",
    "\n",
    "text.on_change('value', update_title)\n",
    "\n",
    "def update_data(attrname, old, new):\n",
    "\n",
    "    # Get the current slider values\n",
    "    q = query.value\n",
    "\n",
    "\n",
    "    # Generate the new curve\n",
    "    x = np.linspace(0, 4*np.pi, N)\n",
    "    y = a*np.sin(k*x + w) + b\n",
    "\n",
    "    source.data = dict(x=x, y=y)\n",
    "\n",
    "for w in [offset, amplitude, phase, freq]:\n",
    "    w.on_change('value', update_data)\n",
    "\n",
    "\n",
    "# Set up layouts and add to document\n",
    "inputs = widgetbox(text, offset, amplitude, phase, freq)\n",
    "\n",
    "curdoc().add_root(row(inputs, plot, width=800))\n",
    "curdoc().title = \"Sliders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
